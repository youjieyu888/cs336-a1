# - Run training with `python -m cs336_basics.train --config default.yml`
# - Run from project root (paths are relative to `cwd`)
# - Will always eval and save checkpoint at the end of training

run:
  run_id: "run-<timestamp>"
  # out_dir: "./out/runs"
  out_dir: "data/runs"
  wandb_project: "cs336-a1"
  # wandb_project: false
  wandb_tags: []

data:
  # train_data_path: "./out/tokens/ts-train/tokens.bin"
  # valid_data_path: "./out/tokens/ts-valid/tokens.bin"
  train_data_path: "ts_train_owt_tokenized"
  valid_data_path: "ts_valid_owt_tokenized"

model:
  d_model: 1280
  num_heads: 16
  d_ff: 3456
  vocab_size: 32_768
  max_seq_len: 256
  num_layers: 12
  theta: 10_000
#  attn_impl: "normal" # "normal", "chunked"
  # attn_chunk_size: 1024


optimizer:
  lr: 5e-4
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01

# (batch_size * max_steps * context_length) ~= 327,680,000 (327.68M)
# e.g. 32 * 40_000 * 256 = 327,680,000
# e.g. 64 * 20_000 * 256 = 327,680,000
# e.g. 128 * 10_000 * 256 = 327,680,000
training:
  use_compile: True
  grad_accum_steps: 1
  batch_size: 32
  max_steps: 13_000
  eval_before_training: false
  eval_interval: 1000
  eval_steps: 50
  eval_batch_size: 128
  checkpoint_interval: 1_000
  max_l2_norm: 1.0
  lr_schedule: "cosine" # "cosine", "linear", "double"
  lr_max: 5.0e-4
  lr_min: 1.0e-4
  warmup_ratio: 0.01 # Only used if `warmup_iters` is not set

  # Only used with other LR schedules
  lr_inter: False # unused with cosine schedule
  warmup_iters: False # overrides `warmup_ratio` if set in inheriting config
  cosine_cycle_iters: False
  linear_cycle_iters: False
  exp_decay_iters: False
  phase_two_iters: False
  phase_two_type: "linear"
